{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59e38a4-3179-40bd-9ca7-5cea75f1bb2a",
   "metadata": {},
   "source": [
    "# **Explain Like I'm Not a Scientist**\n",
    "### *An exploration of (not so) scientific communication*\n",
    "| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|Emily K. Sanders| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |Project 3: NLP|\n",
    "|DSB-318| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |May 3, 2024|\n",
    "---\n",
    "###### *A report for the 2024 Greater Lafayette Association for Data Science Conference on Activism for a Thriving Society*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e7d63-2fec-4bf3-8295-4774a33ee9cc",
   "metadata": {},
   "source": [
    "## Notebook 1 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e21cb0-2e72-4f41-8a76-d73f916e1595",
   "metadata": {},
   "source": [
    "In notebook 1, I introduced the purpose of this work and summarized relevant background information.  I then gave an overview of the method and apparatuses.\n",
    "\n",
    "In this notebook, I will demonstrate how I scraped the data from reddit, including `python` code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d5d0f-9aa7-4225-b9bd-744342d03d2b",
   "metadata": {},
   "source": [
    "## Method: Scraping procedure for posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616c9a4-c4c1-47a3-bde9-b1c5510cf252",
   "metadata": {},
   "source": [
    "Below is the syntax I used to scrape the main feeds of each subreddit.  The comments on these posts had to be scraped separately, and will be addressed in the next notebook.\n",
    "\n",
    "Note: The code below is presented in a Jupyter notebook for the purposes of readability and easy distribution.  In my behind-the-scenes work, all of this was done in `.py` text files.  I encourage anyone looking to reproduce this work to do the same, so that the entire code can be easily executed at the push of a button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83234ed-dbb8-4f3f-933b-41625d515e90",
   "metadata": {},
   "source": [
    "Much of the syntax on this page has been borrowed from a [notebook by Alanna Besaw](https://git.generalassemb.ly/dsb-318/breakfast-hour/tree/master/6_week/wed).  Thanks, Alanna!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c961b-28db-4f82-9bef-727ed7a40e15",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b749ee-02a9-41c4-b46b-f57ab8d9e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import getpass\n",
    "from datetime import date, time, datetime\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dca6c9-9eb4-46b2-a44c-8ffd4f5ca08a",
   "metadata": {},
   "source": [
    "### Getting Authorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2564e-6491-4c75-a4d9-1379b0b2cc77",
   "metadata": {},
   "source": [
    "The first step towards scraping reddit was to set up [an application](https://www.reddit.com/prefs/apps) that can interface with the API.  That application then assigned me several authorization keys that, along with my login information, I could use to connect to the API and request my data.\n",
    "\n",
    "Because my keys and login information are specific to me, it is not possible to create reproducible code for this portion of the script.  Anyone wishing to replicate my work must obtain and enter their own credentials.  I have included `getpass` cells below where this information can be entered without it being visible in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef381834-36af-4be5-9acc-11dd0a51273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter authorization keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6f462ff-1f68-460c-975e-39a83ce21edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "client_id = getpass.getpass() # Listed as \"personal use script\" in your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e87ece17-8ccc-49cb-a965-feb54d406e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "client_secret = getpass.getpass() # Listed as \"secret\" in your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "272ad7a7-6f25-4b50-b7b5-cadff4f5d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "user_agent = getpass.getpass() # The name of your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6c10e33-4b50-43bd-85bc-5078728e698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "username = getpass.getpass() # The reddit username associated with your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e46a8aa-d316-4437-9b50-861b46f0941d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "password = getpass.getpass() # The reddit password associated with your application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6327b1f-4d34-4647-950a-337db399bcc9",
   "metadata": {},
   "source": [
    "Once those credentials were defined, it was necessary to request authorization to access the API, provide some more information, and receive an access token from the API.  Successful connections report a `200` status.  In the code below, I have used a boolean `True`/`False` evaluation within a print statement to provide a more readable status update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d21db33c-c41a-41aa-83ab-4ae53b748158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial hook-in was successful? True\n"
     ]
    }
   ],
   "source": [
    "# Authorize\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "\n",
    "# Set up authorization dictionary\n",
    "data = {\n",
    "  'grant_type': 'password','username': username, 'password': password}\n",
    "\n",
    "# Create a header for scrapes - please change this value if replicating!\n",
    "headers = {'User-Agent': 'EKS-DSB-318/Project-3'}\n",
    "\n",
    "# Connect to the reddit API\n",
    "res = requests.post(\n",
    "    'https://www.reddit.com/api/v1/access_token',\n",
    "    auth=auth, data=data, headers=headers)\n",
    "\n",
    "# Check the API connection\n",
    "print(f'The initial hook-in was successful? {res.status_code == 200}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2314d-d1c8-45fe-a794-9115d78208c9",
   "metadata": {},
   "source": [
    "These initial connections provide a token to authorize further connection and retrieval of data.  When this token is retrieved correctly, it also provides a `200` code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214a2802-96c8-4472-821f-7217104c7318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token is retrieved? True\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the access token\n",
    "token = res.json()['access_token']\n",
    "\n",
    "# Add the token to the headers\n",
    "headers['Authorization'] = f'bearer {token}'\n",
    "\n",
    "# Check that the token works\n",
    "print(\n",
    "  f'''The token is retrieved? {requests.get(\n",
    "  'https://oauth.reddit.com/api/v1/me', headers=headers).status_code == 200}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed953b4e-0e4f-4fde-b152-cd387ba3184d",
   "metadata": {},
   "source": [
    "It was then necessary for me to define some variables to make the scraping process smoother.  In the next cell, I defined the paths to the subreddits, which were the same in every scrape.  In the following cells, I entered the `after` parameter from the previous day's scraping, which tells the API where to pick up so as to minimize the number of duplicate posts.  Because reddit limits the number of posts an application can scrape per day, it was important to save these `after` values at the end of each day of scraping, and update them before resuming the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae2a860-2875-415b-b3e3-d6abc3d5c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define things for the requests - static\n",
    "base_url = 'https://oauth.reddit.com'\n",
    "subreddit1 = '/r/explainlikeimfive' \n",
    "subreddit2 = '/r/askphysics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b41b7242-7b9e-4ebe-8472-d1e977891794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter eli5 after code: \n",
      " t3_1cdeqbx\n"
     ]
    }
   ],
   "source": [
    "# Define things for the requests - varied day-to-day\n",
    "after_eli5 = input('Enter eli5 after code: \\n')\n",
    "# For an example, use 't3_1cdeqbx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d35dc7a8-72b1-4423-8dad-12474807ea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter askp after code: \n",
      " t3_1cd2wg4\n"
     ]
    }
   ],
   "source": [
    "# Define things for the requests - varied day-to-day\n",
    "after_askp = input('Enter askp after code: \\n')\n",
    "# For an example, use 't3_1cd2wg4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b88bd-bcaa-4dad-81eb-0cab2cb61e5f",
   "metadata": {},
   "source": [
    "## Posts: Data In"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f635540-ec3d-4da1-8de8-b6b085809c80",
   "metadata": {},
   "source": [
    "I conducted my scraping via the use of a `while` loop, as shown below.  I first constructed two empty lists to hold the results of each scrape, set a counter to `0`, then, with each repetition of the loop, and for each subreddit: \n",
    "1. (re)define the parameters of each request, either from the inputted value above or with a value extracted from the previous scrape,\n",
    "2. use `requests.get` to scrape the data from the API,\n",
    "3. use `.json()` to convert the raw `JSON` object to a dictionary (full of other, nested dictionaries),\n",
    "4. append the dictionary to the list\n",
    "5. extract the `after` value and assign it to a variable (this variable appears in step 1),\n",
    "6. move the counter, and finally,\n",
    "7. wait 5 seconds before the next request, so as to not overburden the API server.\n",
    "\n",
    "The purpose of this approach was to repeatedly gather posts at the maximum allowable rate until my daily limit of posts was used up accomplishable via setting the `limit` parameter to 100 (reddit's maximum) for each subreddit per loop (i.e., 200 total), and repeating the loop until it reached per-day post limit, which is 1000 posts per day, per subreddit.\n",
    "\n",
    "Unfortunately, in the spirit of transparent science, I must here admit to two errors.  First, as such a novice to reddit, I misinterpreted the 1000 post limit as an overall cap, not a per-subreddit cap, and limited myself to 500 posts per subreddit per day for a while. This is regrettable, as it means I lost out on half of my potential data from those days.  Compounding this, a typographical error in the first version of the code below assigned the `after` value from AskPhysics to both `after_askp` and `after_eli5`, instead of assigning each variable the `after` value corresponding to its own subreddit, as would have been correct.  Combined, by the time I discovered them, these errors had resulted me collecting only a little over 1000 posts for AskPhysics, rather than the 2000 I could have had, and a mere 561 for eli5.  I paused collection of AskPhysics posts so as to not further imbalance the classes, and spent several days trying to make up the gap by scraping more eli5 posts, but this resulted in mostly duplicate posts.  In the interest of not storaging more data than necessary, when writing my data to CSV files, I descended to the level within the nested dictionaries that contained the content of the posts, and discarded the rest - unfortunately, including the `after` values that I erroneously believed to have already been used.  This would not have been an error had the code been written correctly, but I certainly would not do it the same way again, nor advise anyone attempting to replicate my work to do so.  Without the correct `after` values, it was all but impossible to find my place in the API and extract new, unique posts.  At the point when I ran out of time to try, I had only 740 unique eli5 posts.  This is less dramatic of a gap (740 to 1028, vs. 561 to 1028), but still imbalanced.  Fortuitously though, as mentioned above, the comments are the more valuable source of data, and they greatly outnumbered their parent posts.  Furthermore, eli5 produced more than twice as many comments as AskPhysics, which more than closed the gap (more on that later).\n",
    "\n",
    "Finally, below is the final version of the scraping code.  As mentioned previously, I did most of my \"behind the scenes\" work with unglamorous text files, where I set the parameters as detailed above.  For the purposes of demonstration, however, I have set them both to 2 below - scrape two posts, two times.  I made this change as to not needlessly burden the server while editing the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "316e6070-41e7-4a0a-abb6-f6ac808a273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "done scraping\n"
     ]
    }
   ],
   "source": [
    "# Create lists to store the scrapes\n",
    "eli5_scrapes = []\n",
    "askp_scrapes = []\n",
    "\n",
    "# Set a counter\n",
    "i = 0 \n",
    "\n",
    "# Set the loop to repeat 2 times (was 5 during data collection, should have been 10)\n",
    "while i<2:\n",
    "    # Show signs of life (this process can take a while)\n",
    "    print(i)\n",
    "    # (Re-)Define the parameter dictionaries\n",
    "    # On the first iteration, 'after' will be the values defined above\n",
    "    params_eli5 = {'limit': 2, 'after': after_eli5} #(was 'limit': 100 during data collection)\n",
    "    params_askp = {'limit': 2, 'after': after_askp} #(was 'limit': 100 during data collection)\n",
    "    # Make the requests\n",
    "    eli5 = requests.get(base_url+subreddit1, headers=headers, params=params_eli5)\n",
    "    askp = requests.get(base_url+subreddit2, headers=headers, params=params_askp)\n",
    "    # Append to lists\n",
    "    askp_scrapes.append(askp.json())\n",
    "    eli5_scrapes.append(eli5.json())\n",
    "    # Update the afters\n",
    "    after_eli5 = eli5.json()['data']['after']\n",
    "    after_askp = askp.json()['data']['after']\n",
    "    # Move the counter\n",
    "    i += 1\n",
    "    # Always tip your servers\n",
    "    time.sleep(5)\n",
    "print('done scraping')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c4a45-0969-4eda-ade2-57a7942b97a5",
   "metadata": {},
   "source": [
    "At the end of running that code each day, I had two lists full of dictionaries, themselves full of more dictionaries, corresponding to about 100 posts.  Before ending the `python` session, I needed to export this data to external files for storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9edb10-b9c3-4e9e-8032-5f65acf6e68a",
   "metadata": {},
   "source": [
    "## Posts: Data Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5eeec4-eb6f-49df-9273-96866cfbb14f",
   "metadata": {},
   "source": [
    "Because the `JSON`-turned-dictionary objects have such a layered structure, it was necessary to coerce them a bit to get them into a clean dataframe format.  Furthermore, because I was doing 5 scrapes per day for 2 subreddits (or at least, so I thought), I needed to be able to do this in a loop rather than one at a time.  I used a `for` loop to accomplish this, but also incorporated some counters for organizational purposes.  These counters are not inherently doing anything in the code; it was simply a strategy for me to keep track of what was going while the loop was running and which data came from which scrape.  I later discovered a way to encode this information during the scraping itself, but to once again be truthful, that epiphany came too late for much of the data.  For most iterations, the date and scrape number were recorded in the file name, and then added as variables later when those files were read back in and concatenated into one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a52efa-5a70-4ea6-a624-b5bfcb6948f2",
   "metadata": {},
   "source": [
    "At the level of the nested dictionaries with the actual data, there were two entries: one, `['data']`, that contained the information I wanted, and another, `['kind']`, that only contained a one-word descriptor of the type of post.  I wanted to preserve both sets of information, but upon conversion to a dataframe, each dictionary became a separate column, and `kind` was the same for every row.  To resolve this, I wrote a function to double check that `kind` was the same in every row (it always was), to add and populate a row for `kind`, and then drop the `kind` column.  This function is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36711056-02b5-4060-991c-017189b72e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a useful function\n",
    "def be_kind(df):\n",
    "    \"\"\"Double check that there's only one 'kind' per scrape,\n",
    "    then streamline the resulting dataframe.\n",
    "    \n",
    "    Arg: {df}, a dataframe created from a scrape from the reddit API\n",
    "    Return: {df}, the same dataframe, altered\n",
    "    Raise: nothing so far! It would raise all sorts of errors if \n",
    "    applied to a different kind of dataframe, though.\"\"\"\n",
    "\n",
    "    # If it's just the one value\n",
    "    if len(df.loc[:,'kind'].unique())==1:\n",
    "        # Create a new row in the 'data' column and populate it\n",
    "        # with whatever's in the 'kind' column\n",
    "        df.loc['kind','data'] = df.loc[:,'kind'].unique()[0]\n",
    "        # Get rid of the ['kind'] column\n",
    "        df.drop(columns = 'kind', inplace = True)       \n",
    "    # If there's more than one (this never happened)\n",
    "    else:\n",
    "        # Tell me, then stop\n",
    "        print(f'multiple kinds in {i}')\n",
    "    # Return\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e01ef-7306-4fe3-90c1-2ca529efe19a",
   "metadata": {},
   "source": [
    "At the end of processing, when each set of scrapes was combined into a CSV, I wanted to save it with a name that recorded the source, date, and scrape number (i.e., which iteration the loop was on) of its creation.  I defined two functions to do this.  The first uses capabilities from the `datetime` module to produce the date and time of its execution in a custom format.  The second function \n",
    "- gathers the production information,\n",
    "- orients the dataframe the correct way,\n",
    "- extracts column names from the first row, which had been an index column before transposing,\n",
    "- assigns column headers,\n",
    "- and saves the CSV to the working directory under the appropriate name, including a timestamp.  \n",
    "\n",
    "These functions are displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9e1ed7-2c59-4c05-8139-4fc90f6abb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-02_h22-m30-s50\n"
     ]
    }
   ],
   "source": [
    "# Define a useful functions\n",
    "def my_date():\n",
    "  return datetime.now().strftime('%Y-%m-%d_h%H-m%M-s%S')\n",
    "print(my_date()) # test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83ff4d24-7d0e-47de-9a86-ba6f3648af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another useful function\n",
    "def post_csvs(l, df, k2, i):\n",
    "    '''Convert the dataframes of scrapes into meaningfully-named CSVs.\n",
    "    Requires `my_date()` to also be defined.\n",
    "  \n",
    "    Arg:\n",
    "        l: the storage list the df came from; a proxy for the subreddit\n",
    "        df: the dataframe to be converted\n",
    "        k2: my per-subreddit counter\n",
    "        i: the individual post counter\n",
    "    Return:\n",
    "        df in the environment\n",
    "        a CSV file saved to the working directory\n",
    "    Raise:\n",
    "        fingers crossed'''\n",
    "    \n",
    "    # Use l to create text names\n",
    "    if l == askp_scrapes:\n",
    "        sub = 'askp'\n",
    "    if l == eli5_scrapes:\n",
    "        sub = 'eli5'\n",
    "    # Bypass that weird copy thing\n",
    "    # Transpose the dataframe so each scrape is a row, not a column\n",
    "    df2 = df.copy(deep = True).transpose()\n",
    "    # Make the first row the column names\n",
    "    df2.columns = list(df2.iloc[0,:])\n",
    "    # Drop an unnecessary column that appears after multiple merges\n",
    "    if i>0:\n",
    "        df2.drop(index = 'key_0', inplace = True)\n",
    "    # Assign meaningful names (source, datetime, scrape iteration) and write to CSV\n",
    "    df2.to_csv(f'../data/output/posts/{sub}_{my_date()}_scrape{k2}.csv')\n",
    "    # Return\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822b5f4-76f2-42f4-a2ce-c2590a94851e",
   "metadata": {},
   "source": [
    "Those two functions are put to use in the following `for` loop.  Within this loop, I:\n",
    "1. selected one of the two lists of scrapes,\n",
    "2. selected one of the scrapes (i.e., iterations of `i` in the above `for` loop) within that list,\n",
    "3. and selected one of the posts within that scrape.  Then, for that post, I:\n",
    "4. dug through the dictionaries to the level with the data,\n",
    "5. assigned that dictionary to a dataframe called `x` (`y`, in posts other than the first),\n",
    "6. applied `be_kind()` to `x` (`y`), and\n",
    "7. printed an update for debugging and \"proof of life.\"\n",
    "\n",
    "I then moved the counter, and looped back up to step 3 and started the next post within the scrape.  All posts that were not the first one in their scrape were assigned to a dataframe named `y` instead of `x`, and then the two dataframes were merged together.  For the second post in a scrape (that is, the first `y`), they had to be merged just on their indices.  This merge created a column called `key_0` out of what had been the original index of `x`, and all subsequent `y`s were merged on that column.\n",
    "\n",
    "After repeating steps 3-7 for each post within the scrape selected at step 2 and merging them all into one dataframe `x`, I applied my function `post_csvs()` to do some final tidying up, generate the file name, and save the file to the working directory.  Then, I directed the loop back up to step 2 to move to the next scrape within the list, and process its posts the same way.  Once all of the scrapes in that list had been processed, I looped back up to step 1 to do the same for the other list of scrapes.  At the end of all of that, I used a print statement to announce that it was done, and report how many posts had been processed.  The code is below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25725fb1-a1dd-4732-bb9a-26a0ee775cf8",
   "metadata": {},
   "source": [
    "**Note:** Because this cell has already been run for demonstration purposes, and thus already produced files in the `data/output/posts` folder, **future cells that draw from that folder will produce different results if run again.**  To replicate the results, please fork the repository, **delete** the contents of `data/output/posts`, and then try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3cd049c-a0c1-4a8f-be5f-260c90e6a0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the end of the first one, x.shape is (105, 1)\n",
      "y.shape for 1 (i = 1) is (105, 1)\n",
      "at the end of the first one, x.shape is (105, 1)\n",
      "y.shape for 3 (i = 1) is (105, 1)\n",
      "at the end of the first one, x.shape is (106, 1)\n",
      "y.shape for 5 (i = 1) is (106, 1)\n",
      "at the end of the first one, x.shape is (106, 1)\n",
      "y.shape for 7 (i = 1) is (106, 1)\n",
      "All done, 8 posts processed\n"
     ]
    }
   ],
   "source": [
    "# Set an overall counter\n",
    "k=0\n",
    "# For each set of scrapes\n",
    "for l in [askp_scrapes, eli5_scrapes]: \n",
    "    # Set a per-subreddit counter\n",
    "    k2=0\n",
    "    # For each scrape\n",
    "    for j in l:\n",
    "        # Count\n",
    "        k2+=1\n",
    "        # For each individual post scraped, note how much \"digging\" occurs here\n",
    "        for i in list(range(len(j['data']['children']))):\n",
    "            # If it's the first one we're processing\n",
    "            if i==0:\n",
    "                # It gets its own dataframe made of its dictionary\n",
    "                x = pd.DataFrame(j['data']['children'][i])\n",
    "                # Handle the 'kind' column\n",
    "                x = be_kind(x)               \n",
    "                # Print information for debugging, signs of life\n",
    "                print(f'at the end of the first one, x.shape is {x.shape}')\n",
    "            # If it's not the first one\n",
    "            else:\n",
    "                # It becomes a dataframe with a different name\n",
    "                y = pd.DataFrame(j['data']['children'][i])\n",
    "                # Handle the 'kind' column\n",
    "                y = be_kind(y)               \n",
    "                # Print information for debugging, signs of life\n",
    "                print(f'y.shape for {k} (i = {i}) is {y.shape}')\n",
    "                # Merge the dataframes together\n",
    "                if i==1:\n",
    "                    x = x.merge(y, how = 'outer', on = x.index, suffixes = [None, f'_{i}'])\n",
    "                elif i>1:\n",
    "                    x = x.merge(\n",
    "                        y, how = 'outer', left_on = 'key_0', right_on = y.index, suffixes = [\n",
    "                            None, f'_{i}'])\n",
    "            # Count\n",
    "            k+=1\n",
    "        # Tidy up the dataframe, write to CSV\n",
    "        x = post_csvs(l, x, k2, i)\n",
    "print(f'All done, {k} posts processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc1571-98b9-4dbf-8834-c672c2e0121e",
   "metadata": {},
   "source": [
    "Below is the resulting dataframe, `x`, at the end of that loop.  Note that it only contains two rows because `x` is overwritten each time the loop cycles through a set of scrapes.  All 4 of the CSVs created in that loop can be found in the `data/output` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "deb21b21-f927-4b79-9cfe-ce08fe6e143e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_is_blocked</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>awarders</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>category</th>\n",
       "      <th>clicked</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>gildings</th>\n",
       "      <th>hidden</th>\n",
       "      <th>hide_score</th>\n",
       "      <th>id</th>\n",
       "      <th>is_created_from_ads_ui</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>likes</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>locked</th>\n",
       "      <th>media</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>media_only</th>\n",
       "      <th>mod_note</th>\n",
       "      <th>mod_reason_by</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>name</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>over_18</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>permalink</th>\n",
       "      <th>pinned</th>\n",
       "      <th>pwls</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>removed_by</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>saved</th>\n",
       "      <th>score</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>selftext</th>\n",
       "      <th>selftext_html</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>title</th>\n",
       "      <th>top_awarded_type</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>treatment_tags</th>\n",
       "      <th>ups</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "      <th>kind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>cyberchief</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_914nb</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1713965901.0</td>\n",
       "      <td>1713965901.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>self.explainlikeimfive</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1cbyfa1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>#014980</td>\n",
       "      <td>Economics</td>\n",
       "      <td>[{'e': 'text', 't': 'Economics'}]</td>\n",
       "      <td>f8e2fe5c-19e0-11e6-981d-0e2b7c7bd64b</td>\n",
       "      <td>Economics</td>\n",
       "      <td>light</td>\n",
       "      <td>richtext</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_1cbyfa1</td>\n",
       "      <td>False</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>/r/explainlikeimfive/comments/1cbyfa1/eli5_why...</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2928</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>t5_2sokd</td>\n",
       "      <td>r/explainlikeimfive</td>\n",
       "      <td>22830639</td>\n",
       "      <td>public</td>\n",
       "      <td>confidence</td>\n",
       "      <td></td>\n",
       "      <td>ELI5: Why are business expenses deductible fro...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>2928</td>\n",
       "      <td>0.89</td>\n",
       "      <td>https://www.reddit.com/r/explainlikeimfive/com...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Cold-Economist2858</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_vjx5tzgoa</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1714152786.0</td>\n",
       "      <td>1714152786.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>self.explainlikeimfive</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1cdrdus</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>#0079d3</td>\n",
       "      <td>Physics</td>\n",
       "      <td>[{'e': 'text', 't': 'Physics'}]</td>\n",
       "      <td>ef508d1e-19e0-11e6-8b82-0eea2757e675</td>\n",
       "      <td>Physics</td>\n",
       "      <td>light</td>\n",
       "      <td>richtext</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_1cdrdus</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>/r/explainlikeimfive/comments/1cdrdus/eli5_wha...</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>When a tank of gas (nitrogen, co2, argon, etc....</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>t5_2sokd</td>\n",
       "      <td>r/explainlikeimfive</td>\n",
       "      <td>22830639</td>\n",
       "      <td>public</td>\n",
       "      <td>confidence</td>\n",
       "      <td></td>\n",
       "      <td>eli5: What's left in spent gas tanks?</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>https://www.reddit.com/r/explainlikeimfive/com...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>t3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       all_awardings allow_live_comments approved_at_utc approved_by archived  \\\n",
       "data              []               False            None        None    False   \n",
       "data_1            []               False            None        None    False   \n",
       "\n",
       "                    author author_flair_background_color  \\\n",
       "data            cyberchief                          None   \n",
       "data_1  Cold-Economist2858                          None   \n",
       "\n",
       "       author_flair_css_class author_flair_richtext author_flair_template_id  \\\n",
       "data                     None                    []                     None   \n",
       "data_1                   None                    []                     None   \n",
       "\n",
       "       author_flair_text author_flair_text_color author_flair_type  \\\n",
       "data                None                    None              text   \n",
       "data_1              None                    None              text   \n",
       "\n",
       "       author_fullname author_is_blocked author_patreon_flair author_premium  \\\n",
       "data          t2_914nb             False                False          False   \n",
       "data_1    t2_vjx5tzgoa             False                False          False   \n",
       "\n",
       "       awarders banned_at_utc banned_by can_gild can_mod_post category  \\\n",
       "data         []          None      None    False        False     None   \n",
       "data_1       []          None      None    False        False     None   \n",
       "\n",
       "       clicked content_categories contest_mode       created   created_utc  \\\n",
       "data     False               None        False  1713965901.0  1713965901.0   \n",
       "data_1   False               None        False  1714152786.0  1714152786.0   \n",
       "\n",
       "       discussion_type distinguished                  domain downs edited  \\\n",
       "data              None          None  self.explainlikeimfive     0  False   \n",
       "data_1            None          None  self.explainlikeimfive     0  False   \n",
       "\n",
       "       gilded gildings hidden hide_score       id is_created_from_ads_ui  \\\n",
       "data        0       {}  False      False  1cbyfa1                  False   \n",
       "data_1      0       {}  False      False  1cdrdus                  False   \n",
       "\n",
       "       is_crosspostable is_meta is_original_content is_reddit_media_domain  \\\n",
       "data               True   False               False                  False   \n",
       "data_1             True   False               False                  False   \n",
       "\n",
       "       is_robot_indexable is_self is_video likes link_flair_background_color  \\\n",
       "data                 True    True    False  None                     #014980   \n",
       "data_1               True    True    False  None                     #0079d3   \n",
       "\n",
       "       link_flair_css_class                link_flair_richtext  \\\n",
       "data              Economics  [{'e': 'text', 't': 'Economics'}]   \n",
       "data_1              Physics    [{'e': 'text', 't': 'Physics'}]   \n",
       "\n",
       "                      link_flair_template_id link_flair_text  \\\n",
       "data    f8e2fe5c-19e0-11e6-981d-0e2b7c7bd64b       Economics   \n",
       "data_1  ef508d1e-19e0-11e6-8b82-0eea2757e675         Physics   \n",
       "\n",
       "       link_flair_text_color link_flair_type locked media media_embed  \\\n",
       "data                   light        richtext  False  None          {}   \n",
       "data_1                 light        richtext  False  None          {}   \n",
       "\n",
       "       media_only mod_note mod_reason_by mod_reason_title mod_reports  \\\n",
       "data        False     None          None             None          []   \n",
       "data_1      False     None          None             None          []   \n",
       "\n",
       "              name no_follow num_comments num_crossposts num_reports over_18  \\\n",
       "data    t3_1cbyfa1     False          697              0        None   False   \n",
       "data_1  t3_1cdrdus      True            9              0        None   False   \n",
       "\n",
       "       parent_whitelist_status  \\\n",
       "data                   all_ads   \n",
       "data_1                 all_ads   \n",
       "\n",
       "                                                permalink pinned pwls  \\\n",
       "data    /r/explainlikeimfive/comments/1cbyfa1/eli5_why...  False    6   \n",
       "data_1  /r/explainlikeimfive/comments/1cdrdus/eli5_wha...  False    6   \n",
       "\n",
       "       quarantine removal_reason removed_by removed_by_category  \\\n",
       "data        False           None       None                None   \n",
       "data_1      False           None       None                None   \n",
       "\n",
       "       report_reasons  saved score secure_media secure_media_embed  \\\n",
       "data             None  False  2928         None                 {}   \n",
       "data_1           None  False     0         None                 {}   \n",
       "\n",
       "                                                 selftext  \\\n",
       "data                                                        \n",
       "data_1  When a tank of gas (nitrogen, co2, argon, etc....   \n",
       "\n",
       "                                            selftext_html send_replies  \\\n",
       "data                                                 None         True   \n",
       "data_1  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...         True   \n",
       "\n",
       "       spoiler stickied          subreddit subreddit_id  \\\n",
       "data     False    False  explainlikeimfive     t5_2sokd   \n",
       "data_1   False    False  explainlikeimfive     t5_2sokd   \n",
       "\n",
       "       subreddit_name_prefixed subreddit_subscribers subreddit_type  \\\n",
       "data       r/explainlikeimfive              22830639         public   \n",
       "data_1     r/explainlikeimfive              22830639         public   \n",
       "\n",
       "       suggested_sort thumbnail  \\\n",
       "data       confidence             \n",
       "data_1     confidence             \n",
       "\n",
       "                                                    title top_awarded_type  \\\n",
       "data    ELI5: Why are business expenses deductible fro...             None   \n",
       "data_1              eli5: What's left in spent gas tanks?             None   \n",
       "\n",
       "       total_awards_received treatment_tags   ups upvote_ratio  \\\n",
       "data                       0             []  2928         0.89   \n",
       "data_1                     0             []     0         0.38   \n",
       "\n",
       "                                                      url user_reports  \\\n",
       "data    https://www.reddit.com/r/explainlikeimfive/com...           []   \n",
       "data_1  https://www.reddit.com/r/explainlikeimfive/com...           []   \n",
       "\n",
       "       view_count visited whitelist_status wls kind  \n",
       "data         None   False          all_ads   6   t3  \n",
       "data_1       None   False          all_ads   6   t3  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24baa1-5067-4433-ac61-ea4d585f9a88",
   "metadata": {},
   "source": [
    "## Posts: Data Back In"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7cd26-7fa9-42ec-9b64-18bb91c355c0",
   "metadata": {},
   "source": [
    "Because each scrape was saved as an individual CSV, it was necessary to concatenate them together for analysis.  I concatenated all the posts together before scraping comments, so that there would only one source of post IDs to iterate through during comment scraping.  \n",
    "\n",
    "To do this, I first read in a list of the names of the CSVs and created a placeholder list to receive them all.  I then used a `for` loop to read in each CSV as a dataframe and create columns within it for the date it was scraped, its scrape number, and its source, which were recorded in the file name in a previous step.  I then appended its contents to the list.  Finally, I concatenated the list into a dataframe.  The code for this process is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d97ab33-f5e4-49be-aab7-87a9ff05b509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 CSVs to do\n"
     ]
    }
   ],
   "source": [
    "# Get a list of the CSVs\n",
    "previous_scrapes = os.listdir('../data/output/posts/')\n",
    "print(f\"{len(previous_scrapes)} CSVs to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "daca7e7b-d9f4-4957-b6e0-235355d3528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a placeholder list\n",
    "scrapes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f032f634-bcc5-4996-aa7e-a5526f9a4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the CSVs\n",
    "for file in previous_scrapes:\n",
    "  path = \"../data/output/\" + file\n",
    "  df = pd.read_csv(path)\n",
    "  file_name = file.split('.')[0] #drop the .csv\n",
    "  file_name = file_name.split('_') #break into chunks\n",
    "  df['source'] = file_name[0]\n",
    "  df['date_scraped'] = file_name[1]\n",
    "  df['scrape_num'] = file_name[-1]\n",
    "  scrapes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de9d2de8-f07b-43ba-a143-b85c3e4018d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapes is now a dataframe of dimensions (8, 110)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_is_blocked</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>awarders</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>category</th>\n",
       "      <th>clicked</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>gildings</th>\n",
       "      <th>hidden</th>\n",
       "      <th>hide_score</th>\n",
       "      <th>id</th>\n",
       "      <th>is_created_from_ads_ui</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>likes</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>locked</th>\n",
       "      <th>media</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>media_only</th>\n",
       "      <th>mod_note</th>\n",
       "      <th>mod_reason_by</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>name</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>over_18</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>permalink</th>\n",
       "      <th>pinned</th>\n",
       "      <th>pwls</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>removed_by</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>saved</th>\n",
       "      <th>score</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>selftext</th>\n",
       "      <th>selftext_html</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>title</th>\n",
       "      <th>top_awarded_type</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>treatment_tags</th>\n",
       "      <th>ups</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "      <th>kind</th>\n",
       "      <th>source</th>\n",
       "      <th>date_scraped</th>\n",
       "      <th>scrape_num</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>CDNZimmy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_5qwkjeko</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.714098e+09</td>\n",
       "      <td>1.714098e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>self.AskPhysics</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1cd9r3s</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_1cd9r3s</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>/r/AskPhysics/comments/1cd9r3s/keplers_constant/</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>I’m looking for an explanation of a weird prob...</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>AskPhysics</td>\n",
       "      <td>t5_2sumo</td>\n",
       "      <td>r/AskPhysics</td>\n",
       "      <td>607255</td>\n",
       "      <td>public</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Keplers constant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.67</td>\n",
       "      <td>https://www.reddit.com/r/AskPhysics/comments/1...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>t3</td>\n",
       "      <td>askp</td>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>scrape1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 all_awardings  allow_live_comments  approved_at_utc  approved_by  \\\n",
       "0       data            []                False              NaN          NaN   \n",
       "\n",
       "   archived    author  author_flair_background_color  author_flair_css_class  \\\n",
       "0     False  CDNZimmy                            NaN                     NaN   \n",
       "\n",
       "  author_flair_richtext  author_flair_template_id  author_flair_text  \\\n",
       "0                    []                       NaN                NaN   \n",
       "\n",
       "   author_flair_text_color author_flair_type author_fullname  \\\n",
       "0                      NaN              text     t2_5qwkjeko   \n",
       "\n",
       "   author_is_blocked  author_patreon_flair  author_premium awarders  \\\n",
       "0              False                 False           False       []   \n",
       "\n",
       "   banned_at_utc  banned_by  can_gild  can_mod_post  category  clicked  \\\n",
       "0            NaN        NaN     False         False       NaN    False   \n",
       "\n",
       "   content_categories  contest_mode       created   created_utc  \\\n",
       "0                 NaN         False  1.714098e+09  1.714098e+09   \n",
       "\n",
       "   discussion_type  distinguished           domain  downs  edited  gilded  \\\n",
       "0              NaN            NaN  self.AskPhysics      0   False       0   \n",
       "\n",
       "  gildings  hidden  hide_score       id  is_created_from_ads_ui  \\\n",
       "0       {}   False       False  1cd9r3s                   False   \n",
       "\n",
       "   is_crosspostable  is_meta  is_original_content  is_reddit_media_domain  \\\n",
       "0              True    False                False                   False   \n",
       "\n",
       "   is_robot_indexable  is_self  is_video  likes link_flair_background_color  \\\n",
       "0                True     True     False    NaN                         NaN   \n",
       "\n",
       "  link_flair_css_class link_flair_richtext link_flair_text  \\\n",
       "0                  NaN                  []             NaN   \n",
       "\n",
       "  link_flair_text_color link_flair_type  locked  media media_embed  \\\n",
       "0                  dark            text   False    NaN          {}   \n",
       "\n",
       "   media_only  mod_note  mod_reason_by  mod_reason_title mod_reports  \\\n",
       "0       False       NaN            NaN               NaN          []   \n",
       "\n",
       "         name  no_follow  num_comments  num_crossposts  num_reports  over_18  \\\n",
       "0  t3_1cd9r3s      False             5               0          NaN    False   \n",
       "\n",
       "  parent_whitelist_status                                         permalink  \\\n",
       "0                 all_ads  /r/AskPhysics/comments/1cd9r3s/keplers_constant/   \n",
       "\n",
       "   pinned  pwls  quarantine  removal_reason  removed_by  removed_by_category  \\\n",
       "0   False     6       False             NaN         NaN                  NaN   \n",
       "\n",
       "   report_reasons  saved  score  secure_media secure_media_embed  \\\n",
       "0             NaN  False      2           NaN                 {}   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  I’m looking for an explanation of a weird prob...   \n",
       "\n",
       "                                       selftext_html  send_replies  spoiler  \\\n",
       "0  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...          True    False   \n",
       "\n",
       "   stickied   subreddit subreddit_id subreddit_name_prefixed  \\\n",
       "0     False  AskPhysics     t5_2sumo            r/AskPhysics   \n",
       "\n",
       "   subreddit_subscribers subreddit_type suggested_sort  thumbnail  \\\n",
       "0                 607255         public            NaN        NaN   \n",
       "\n",
       "              title  top_awarded_type  total_awards_received treatment_tags  \\\n",
       "0  Keplers constant               NaN                      0             []   \n",
       "\n",
       "   ups  upvote_ratio                                                url  \\\n",
       "0    2          0.67  https://www.reddit.com/r/AskPhysics/comments/1...   \n",
       "\n",
       "  user_reports  view_count  visited whitelist_status  wls kind source  \\\n",
       "0           []         NaN    False          all_ads    6   t3   askp   \n",
       "\n",
       "  date_scraped scrape_num link_flair_template_id  \n",
       "0   2024-05-02    scrape1                    NaN  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate into one dataframe\n",
    "scrapes = pd.concat(scrapes, sort=False, axis=0, join='outer', ignore_index=True)\n",
    "print(f'scrapes is now a dataframe of dimensions {scrapes.shape}')\n",
    "scrapes.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53739bd-4543-4c98-a5e2-d102276620d3",
   "metadata": {},
   "source": [
    "Once all of the posts were concatenated into one dataframe, I conducted some preliminary exploratory data analysis, which I will briefly summarize in that section.  The final step of processing this data, was to save it back out as a combined CSV.  That CSV can be found the `data/input` folder, but because of its size, it could not easily be pushed to GitHub for distribution.  Therefore, this CSV, as well as concatenated CSVs for each subreddit's comments, have been compressed into a zip file, called `concatted-scrapes-separate-csvs-by-source.zip`.  Within that file, this CSV is called `combined-as-of_2024-04-30_h17-m21-s57.csv`.\n",
    "\n",
    "For the purposes of demonstration, I will save the sample scrapes generated above into a CSV in the `data/output` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a02e03a-4897-4ee3-a3b9-a5af1bb2b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the giant df as csv\n",
    "scrapes.to_csv(f'../data/output/concatted-wholes/combined-as-of_{my_date()}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d465eb-bea0-44b9-8365-178b7a53bcba",
   "metadata": {},
   "source": [
    "In this notebook, I have walked through my process for scraping, saving, and processing posts from each of my chosen subreddits.  In the next notebook, I will explain this process for comments, and how I then combined posts and comments into one dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
